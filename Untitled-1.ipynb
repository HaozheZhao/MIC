{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T5 based model\n",
    "from model.instructblip import InstructBlipConfig, InstructBlipModel, InstructBlipPreTrainedModel,InstructBlipForConditionalGeneration,InstructBlipProcessor\n",
    "import datasets\n",
    "import json\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import torch\n",
    "model_type=\"instructblip\"\n",
    "model_ckpt=\"/home/haozhezhao/MMICL-Instructblip-T5-xxl\"\n",
    "processor_ckpt = \"Salesforce/instructblip-flan-t5-xxl\"\n",
    "config = InstructBlipConfig.from_pretrained(model_ckpt )\n",
    "\n",
    "if 'instructblip' in model_type:\n",
    "    model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "        model_ckpt,\n",
    "        config=config).to('cuda:0',dtype=torch.bfloat16) \n",
    "\n",
    "image_palceholder=\"å›¾\"\n",
    "sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n",
    "processor = InstructBlipProcessor.from_pretrained(\n",
    "    processor_ckpt\n",
    ")\n",
    "sp = sp+processor.tokenizer.additional_special_tokens[len(sp):]\n",
    "processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n",
    "if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n",
    "    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n",
    "replace_token=\"\".join(32*[image_palceholder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x6=18\"\n"
     ]
    }
   ],
   "source": [
    "image = Image.open (\"images/cal_num1.png\")\n",
    "image1 = Image.open (\"images/cal_num2.png\")\n",
    "image2 = Image.open (\"images/cal_num3.png\")\n",
    "images = [image,image1,image2]\n",
    "\n",
    "prompt = [f'Use the image 0: <image0>{replace_token},image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aid to help you calculate the equation accurately. image 0 is 2+1=3.\\nimage 1 is 5+6=11.\\nimage 2 is\"']\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a flamingo. They are native to South America and are known for their bright red plumage and distinctive call.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image = Image.open (\"images/chinchilla.png\")\n",
    "image1 = Image.open (\"images/shiba.png\")\n",
    "image2 = Image.open (\"images/flamingo.png\")\n",
    "images = [image,image1,image2]\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'image 0 is <image0>{replace_token},image 1 is <image1>{replace_token},image 2 is <image2>{replace_token}. Question: <image0> is a chinchilla. They are mainly found in Chile.\\n Question: <image1> is a shiba. They are very popular in Japan.\\nQuestion: image 2 is']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0 is a photo of a flamingo standing in the water, image 1 is a cartoon drawing of a flamingo and image 2 is a low polygon count 3d model animation render of\n"
     ]
    }
   ],
   "source": [
    "image = Image.open (\"images/flamingo_photo.png\")\n",
    "image1 = Image.open (\"images/flamingo_cartoon.png\")\n",
    "image2 = Image.open (\"images/flamingo_3d.png\")\n",
    "\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aids to help you answer the question. Question: Give the reason why image 0, image 1 and image 2 are different? Answer:']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=80,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
